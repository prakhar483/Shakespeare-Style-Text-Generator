# ğŸ­ Shakespeare Text Generation using RNN

Generate Shakespeare-style text by training a character-level Recurrent Neural Network (RNN) on Shakespeareâ€™s writings. The model learns sequential patterns to create coherent and stylistically similar text passages.

The repository includes a full workflow notebook and a trained model for easy experimentation.

---

## ğŸ“ Project Folder Structure

ğŸ“¦ Shakespeare-Text-Generation-RNN
- â”‚
- â”œâ”€â”€ Rnn_Project2_Text_Generation_Shakespeare.ipynb # Notebook containing full workflow:
- â”‚ # data preprocessing, training, evaluation, text generation
- â”‚
- â”œâ”€â”€ lstm_shakespeare_model.pth # Trained LSTM model saved after training
- â”‚
- â””â”€â”€ README.md # Project documentation


---

## ğŸ“ Problem Statement

Build a deep learning model to generate Shakespeare-style text using a character-level RNN.  
The model learns from sequences of characters to predict the next character, enabling it to produce novel text passages that mimic Shakespeareâ€™s writing style.

---

## ğŸ“š Dataset Details

**Source:** Tiny Shakespeare dataset  
**Format:** Plain text (~1MB)

**Characteristics:**
- Character-level data (letters, punctuation, spaces)
- Rich literary style and diverse vocabulary

---

## ğŸ› ï¸ Model & Approach

- Character-level RNN trained to predict the next character from previous sequences  
- **Architectures compared:** SimpleRNN, LSTM, GRU  
- **Loss Function:** Cross-Entropy Loss  
- **Training:** Multiple epochs with sequence-based input-target pairs  
- **Text Generation:** Sampling techniques with temperature scaling  
- **Evaluation:** Train/validation loss + qualitative inspection of generated text  

---

## ğŸ¯ Key Highlights

- Generated text mimics Shakespeareâ€™s style and rhythm  
- Demonstrated the RNNâ€™s ability to learn long-range dependencies  
- Strong foundation for sequence modeling in NLP  
- Shows the creative potential of deep learning models  

---

## ğŸ” Model Performance

| Model      | Train Loss | Validation Loss |
|------------|------------|-----------------|
| SimpleRNN  | 1.2820     | 1.2842          |
| LSTM       | 0.9914     | 0.9977          |
| GRU        | 1.1040     | 1.1320          |

**Observations:**
- **LSTM** achieves the best train/validation loss â†’ best for character-level generation  
- **GRU** performs better than SimpleRNN but worse than LSTM  
- SimpleRNN struggles with long-term dependencies  

---

## ğŸ“Œ Conclusion & Insights

- Character-level LSTM effectively captures long-term dependencies in Shakespearean text  
- Generated text showcases Shakespeare-like vocabulary and rhythm  
- Provides a strong base for advanced NLP projects such as Transformers  

---

## ğŸš€ Potential Improvements

### ğŸ“Š Model & Architecture
- Shift to word-level modeling  
- Try deeper or stacked GRUs  
- Explore Transformer-based architectures  
- Add embedding layers for dense character representations  

### ğŸ”§ Hyperparameter Optimization
- Tune sequence length, hidden units, layers  
- Adjust batch size, learning rate  
- Extend epochs and use LR schedulers  

### ğŸ¨ Sampling & Generation
- Add **Top-K**, **Top-P (Nucleus) Sampling**  
- Improve coherence with temperature tuning  

---

## âš ï¸ Limitations

- High computational cost  
- Long text coherence still challenging  
- Memory usage grows with sequence length  
- Style limited to training corpus  

---

## âœ… Key Takeaway

This project demonstrates the core principles of sequence modeling and natural language generation using LSTMs:

- Prepares text for next-character prediction  
- Builds and trains memory-capable LSTM networks  
- Implements temperature-based sampling for creative text generation  

The LSTM model establishes the foundation for future improvements using word-level models, attention mechanisms, and Transformers.

---
